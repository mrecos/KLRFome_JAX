# KLRfome Python/JAX Implementation - Cursor Rules

## Project Overview

This is the Python/JAX implementation of **KLRfome** (Kernel Logistic Regression on Focal Mean Embeddings), refactored from the original R package. KLRfome solves a Distribution Regression problem for geographic/spatial prediction.

**Repository**: https://github.com/mrecos/KLRFome_JAX
**Original R Package**: https://github.com/mrecos/klrfome
**Original Paper**: Harris, M.D. (2019). KLRfome - Kernel Logistic Regression on Focal Mean Embeddings. Journal of Open Source Software, 4(35), 722.

## Core Concept

KLRfome predicts binary outcomes (e.g., site present/absent) across landscapes by:
1. Representing each site as a **distribution** of environmental feature vectors (not a single point)
2. Computing similarity between distributions using **mean embeddings in RKHS**
3. Fitting **Kernel Logistic Regression** on the similarity matrix
4. Predicting using **focal windows** that compute similarity between landscape neighborhoods and training data

## Current Implementation Status

### âœ… Completed (Phase 1 & 2)
- **Data Structures**: `SampleCollection`, `TrainingData`, `RasterStack`
- **Kernels**: 
  - Exact RBF kernel (`RBFKernel`)
  - Random Fourier Features approximation (`RandomFourierFeatures`)
  - Mean Embedding Kernel (`MeanEmbeddingKernel`)
  - **Sliced Wasserstein Kernel** (`WassersteinKernel`) - captures distributional shape
- **KLR Model**: `KernelLogisticRegression` with IRLS algorithm
- **Focal Prediction**: `FocalPredictor` and `WassersteinFocalPredictor`
- **High-Level API**: `KLRfome` class with `kernel_type` parameter
- **R Validation**: Core algorithms validated to match R implementation exactly

### ðŸš§ In Progress / Next Steps
- **Phase 2**: Performance optimizations (multi-device parallel prediction)
- **Phase 3**: Usability features (I/O integration, visualization, cross-validation)
- **Phase 4**: Extensions (additional kernels, multi-scale windows, model serialization)

## Technical Architecture

### Key Technologies
- **JAX**: For GPU acceleration, automatic differentiation, and JIT compilation
- **JAX Idioms**: Use `@jit`, `vmap`, and functional style throughout
- **Type Hints**: Use `jaxtyping` for array shape annotations
- **Geospatial**: `rasterio` for rasters, `geopandas` for vector data

### Code Style
- Follow PEP 8 with 100 character line length
- Use type hints with `jaxtyping` for arrays
- Prefer functional style compatible with JAX
- Avoid Python loops where vectorization is possible
- Use `jax.jit` for hot paths, `jax.vmap` for batch operations

### Important Design Decisions
1. **JIT Compatibility**: All JIT-compiled functions must use JAX arrays, not Python objects
2. **Static Arguments**: Use `static_argnames` for JIT, not `static_argnums` with `self`
3. **Memory Efficiency**: RFF approximation avoids large kernel matrices
4. **Numerical Stability**: IRLS uses ridge regularization, convergence checks

## R/Python Alignment - Critical Learnings

The Python implementation was validated against R. Key alignment points:

### IRLS Algorithm (klrfome/models/klr.py)
- **Formulation**: `(K + Î»Â·diag(1/W))Â·Î± = z` - matches R exactly
- **Initial alpha**: `jnp.ones(n) / n` (NOT zeros - this affects convergence)
- **Convergence criterion**: `jnp.all(jnp.abs(alpha_new - alpha) <= tol)`
- **Default tolerance**: `tol = 0.001` (matches R)
- **No probability clipping**: During IRLS, don't clip probabilities (R doesn't)

### Sigmoid Function
- Use simple sigmoid: `1 / (1 + jnp.exp(-x))`
- Do NOT use "numerically stable" versions during IRLS (causes divergence from R)
- Clipping can be used at boundaries but not during iteration

### Kernel Computation (klrfome/kernels/distribution.py)
- **Rounding**: Use `round_kernel=True, kernel_decimals=3` to match R's default
- This affects similarity matrix values and downstream alpha/predictions
- R's kernel computation rounds intermediate results

### Coordinate Systems (klrfome/io/vector.py)
- **CRITICAL**: `rasterio.transform.rowcol(transform, x, y)` returns `(row, col)` not `(col, row)`
- Getting this wrong causes samples to be extracted from wrong locations
- Always assign as: `row, col = rasterio.transform.rowcol(transform, x, y)`

### Default Parameters
- **sigma**: 0.5 (kernel bandwidth)
- **lambda_reg**: 0.1 (regularization strength)
- **kernel_type**: 'mean_embedding' (default) or 'wasserstein'
- **n_rff_features**: 256 for fast approximation, 0 for exact kernel (matches R)
- **n_projections**: 100 (for Wasserstein kernel only)
- **wasserstein_p**: 2 (order of Wasserstein distance, 1 or 2)

## Wasserstein Kernel

The Wasserstein kernel (`kernel_type='wasserstein'`) provides shape-aware distribution comparison:

### When to Use Wasserstein
- Distributions have similar means but different shapes (bimodal vs unimodal)
- Multimodal environmental signatures (e.g., edge effects)
- Shape/structure is important for discrimination

### When to Use Mean Embedding (Default)
- Distributions differ primarily by mean/location
- Need faster computation (RFF approximation)
- Need R compatibility

### Wasserstein Parameters
```python
model = KLRfome(
    sigma=0.5,
    kernel_type='wasserstein',
    n_projections=100,    # More = better approximation, slower
    wasserstein_p=2       # p=2 standard, p=1 more robust to outliers
)
```

### Sliced Wasserstein Distance
The implementation uses Sliced Wasserstein distance (O(L Ã— n log n)) which:
- Projects distributions onto random 1D subspaces
- Computes exact 1D Wasserstein via sorting
- Averages over all projections (L = n_projections)

### Data Scaling
- Z-score normalization: `(x - mean) / std`
- Apply to both training samples AND raster data before prediction
- Use same mean/std computed from training data

## Validation Workflow

Run validation against R:
```bash
# 1. Generate benchmark data
python benchmarks/generate_benchmark_data.py

# 2. Run R export
Rscript benchmarks/validate_r_export.R

# 3. Validate Python against R
python benchmarks/validate_against_r.py
```

All three components should match: Kernel Matrix, Alpha Values, Predictions.

## File Structure

```
klrfome/
â”œâ”€â”€ __init__.py          # Package exports
â”œâ”€â”€ api.py               # High-level KLRfome class (with kernel_type parameter)
â”œâ”€â”€ data/                # Data structures (SampleCollection, TrainingData, RasterStack)
â”œâ”€â”€ kernels/
â”‚   â”œâ”€â”€ rbf.py           # Exact RBF kernel
â”‚   â”œâ”€â”€ rff.py           # Random Fourier Features approximation
â”‚   â”œâ”€â”€ distribution.py  # Mean Embedding Kernel
â”‚   â””â”€â”€ wasserstein.py   # Sliced Wasserstein Distance & Kernel
â”œâ”€â”€ models/              # KLR model with IRLS
â”œâ”€â”€ prediction/
â”‚   â””â”€â”€ focal.py         # FocalPredictor & WassersteinFocalPredictor
â”œâ”€â”€ io/                  # Raster/vector I/O (rasterio, geopandas)
â”œâ”€â”€ utils/               # GPU detection, scaling, validation, serialization
â””â”€â”€ visualization/       # Plotting utilities

tests/
â”œâ”€â”€ test_wasserstein.py  # Wasserstein kernel tests (31 tests)
â””â”€â”€ ...                  # Other test files

benchmarks/
â”œâ”€â”€ compare_kernels.py   # Mean embedding vs Wasserstein comparison
â”œâ”€â”€ validate_against_r.py
â””â”€â”€ ...
```

## Testing Requirements

- **Unit Tests**: All core functionality (kernels, KLR, prediction, data structures)
- **Integration Tests**: End-to-end workflows
- **Property-Based Testing**: Use Hypothesis for numerical properties
- **Coverage Target**: 80%+ for core modules

Run tests: `pytest tests/ --cov=klrfome`

## Common Patterns

### Kernel Implementation
```python
from jax import jit
import jax.numpy as jnp

@jit
def __call__(self, X, Y):
    # Kernel computation
    return jnp.array(...)
```

### JIT-Compatible Batch Processing
```python
@staticmethod
@partial(jit, static_argnames=['window_size'])
def _predict_batch(padded_data, coords, pad, window_size, ...):
    # Extract needed values, don't use self
    def predict_single(coord):
        # Process single item
    return vmap(predict_single)(coords)
```

## Known Issues / Limitations

1. **Exact Kernels in JIT**: FocalPredictor handles exact kernels via `_predict_batch_exact` (not JIT-compiled)
2. **I/O Modules**: Placeholder implementations, need full rasterio/geopandas integration
3. **Cross-Validation**: Placeholder, needs full k-fold implementation
4. **Model Serialization**: Placeholder, needs pickle/JAX serialization

## Development Priorities

1. **Complete I/O Integration**: Full rasterio/geopandas support for real data workflows
2. **Tutorial Notebooks**: Create `01_quickstart.ipynb` and other tutorials
3. **Cross-Validation**: Implement k-fold CV with stratification
4. **Documentation**: API docs, user guide, theory guide
5. **Performance**: Multi-device parallel prediction, benchmarking

## References

- Full technical specification: `AI_Context/klrfome_python_refactor_spec.md`
- Original R package documentation and paper for algorithm details
- JAX documentation for JIT, vmap, and best practices
- Validation scripts: `benchmarks/validate_against_r.py`

## Notes for AI Assistant

- Prioritize correctness over cleverness for numerical algorithms
- Use established formulations for IRLS and kernel computations
- Test that kernels satisfy mathematical properties (symmetry, PSD, etc.)
- Be mindful of memory for large kernel matrices (prefer RFF)
- When debugging Python vs R differences, use the validation workflow
- Remember: `rowcol` returns `(row, col)` - a common source of bugs
- When in doubt, refer to the technical specification document
