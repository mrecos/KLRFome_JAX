{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Tuning with Cross-Validation\n",
        "\n",
        "This notebook demonstrates how to use cross-validation to select optimal hyperparameters for KLRfome models. We'll explore:\n",
        "\n",
        "1. Basic cross-validation usage\n",
        "2. Grid search over parameter space\n",
        "3. Model selection based on CV results\n",
        "4. Understanding validation metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from rasterio.transform import from_bounds\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "from klrfome import KLRfome\n",
        "from klrfome.data.simulation import create_simulated_raster_stack\n",
        "from klrfome.visualization import plot_roc_curve\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Test Dataset\n",
        "\n",
        "We'll create a dataset with clear separation between sites and background for demonstration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raster: (3, 80, 80)\n",
            "Sites: 15\n"
          ]
        }
      ],
      "source": [
        "# Create raster stack\n",
        "raster_stack = create_simulated_raster_stack(cols=80, rows=80, n_bands=3, seed=SEED)\n",
        "\n",
        "# Create site locations\n",
        "n_sites = 15\n",
        "site_points = [Point(np.random.uniform(0.1, 0.9), np.random.uniform(0.1, 0.9)) \n",
        "               for _ in range(n_sites)]\n",
        "sites_gdf = gpd.GeoDataFrame(geometry=site_points, crs=raster_stack.crs)\n",
        "\n",
        "print(f\"Raster: {raster_stack.data.shape}\")\n",
        "print(f\"Sites: {len(sites_gdf)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Basic Cross-Validation\n",
        "\n",
        "Use the built-in `cross_validate()` method to evaluate model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data: 55 locations\n",
            "  Sites: 15\n",
            "  Background: 40\n"
          ]
        }
      ],
      "source": [
        "# Prepare training data\n",
        "model = KLRfome(sigma=1.0, lambda_reg=0.1, n_rff_features=256, seed=SEED)\n",
        "training_data = model.prepare_data(\n",
        "    raster_stack=raster_stack,\n",
        "    sites=sites_gdf,\n",
        "    n_background=40,\n",
        "    samples_per_location=20\n",
        ")\n",
        "\n",
        "print(f\"Training data: {training_data.n_locations} locations\")\n",
        "print(f\"  Sites: {training_data.n_sites}\")\n",
        "print(f\"  Background: {training_data.n_background}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "KLRfome.cross_validate() got an unexpected keyword argument 'seed'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run cross-validation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m cv_results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstratified\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCross-validation completed:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Number of folds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_results[\u001b[33m'\u001b[39m\u001b[33mn_folds\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: KLRfome.cross_validate() got an unexpected keyword argument 'seed'"
          ]
        }
      ],
      "source": [
        "# Run cross-validation\n",
        "cv_results = model.cross_validate(\n",
        "    training_data=training_data,\n",
        "    n_folds=5,\n",
        "    stratified=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "print(f\"Cross-validation completed:\")\n",
        "print(f\"  Number of folds: {cv_results['n_folds']}\")\n",
        "print(f\"  Mean train size: {cv_results['mean_train_size']:.1f}\")\n",
        "print(f\"  Mean test size: {cv_results['mean_test_size']:.1f}\")\n",
        "print(f\"  Best fold: {cv_results['best_fold']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display metrics for each fold\n",
        "print(\"\\nPer-fold metrics:\")\n",
        "print(\"-\" * 80)\n",
        "for fold_result in cv_results['folds']:\n",
        "    metrics = fold_result['metrics']\n",
        "    print(f\"Fold {fold_result['fold']}:\")\n",
        "    print(f\"  Train: {fold_result['n_train']}, Test: {fold_result['n_test']}\")\n",
        "    print(f\"  AUC: {metrics['AUC']:.3f}\")\n",
        "    print(f\"  Accuracy: {metrics['Accuracy']:.3f}\")\n",
        "    print(f\"  Sensitivity: {metrics['Sensitivity']:.3f}\")\n",
        "    print(f\"  Specificity: {metrics['Specificity']:.3f}\")\n",
        "    print(f\"  Kappa: {metrics['Kappa']:.3f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display aggregated metrics\n",
        "print(\"Aggregated metrics (mean ± std across folds):\")\n",
        "print(\"-\" * 80)\n",
        "agg = cv_results['aggregated_metrics']\n",
        "key_metrics = ['AUC', 'Accuracy', 'Sensitivity', 'Specificity', 'Kappa', 'Precision', 'F_Measure']\n",
        "for metric in key_metrics:\n",
        "    mean_key = f'{metric}_mean'\n",
        "    std_key = f'{metric}_std'\n",
        "    if mean_key in agg and std_key in agg:\n",
        "        print(f\"  {metric:15s}: {agg[mean_key]:.3f} ± {agg[std_key]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Grid Search Over Parameter Space\n",
        "\n",
        "Systematically search over combinations of `sigma` and `lambda_reg` to find optimal hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameter grid\n",
        "sigma_values = [0.5, 1.0, 2.0, 3.0]\n",
        "lambda_values = [0.01, 0.1, 1.0]\n",
        "\n",
        "print(f\"Grid search over {len(sigma_values)} × {len(lambda_values)} = {len(sigma_values) * len(lambda_values)} combinations\")\n",
        "print(f\"Sigma values: {sigma_values}\")\n",
        "print(f\"Lambda values: {lambda_values}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run grid search\n",
        "grid_results = []\n",
        "\n",
        "for sigma in sigma_values:\n",
        "    for lambda_reg in lambda_values:\n",
        "        print(f\"\\nTesting sigma={sigma:.2f}, lambda={lambda_reg:.2f}...\")\n",
        "        \n",
        "        # Create model with these parameters\n",
        "        model_grid = KLRfome(\n",
        "            sigma=sigma,\n",
        "            lambda_reg=lambda_reg,\n",
        "            n_rff_features=256,\n",
        "            window_size=3,\n",
        "            seed=SEED\n",
        "        )\n",
        "        \n",
        "        # Run cross-validation\n",
        "        cv_result = model_grid.cross_validate(\n",
        "            training_data=training_data,\n",
        "            n_folds=5,\n",
        "            stratified=True,\n",
        "            seed=SEED\n",
        "        )\n",
        "        \n",
        "        # Store results\n",
        "        grid_results.append({\n",
        "            'sigma': sigma,\n",
        "            'lambda': lambda_reg,\n",
        "            'mean_auc': cv_result['aggregated_metrics']['AUC_mean'],\n",
        "            'std_auc': cv_result['aggregated_metrics']['AUC_std'],\n",
        "            'mean_accuracy': cv_result['aggregated_metrics']['Accuracy_mean'],\n",
        "            'std_accuracy': cv_result['aggregated_metrics']['Accuracy_std'],\n",
        "            'mean_kappa': cv_result['aggregated_metrics']['Kappa_mean'],\n",
        "        })\n",
        "        \n",
        "        print(f\"  Mean AUC: {cv_result['aggregated_metrics']['AUC_mean']:.3f} ± {cv_result['aggregated_metrics']['AUC_std']:.3f}\")\n",
        "\n",
        "print(f\"\\n✓ Grid search completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame for easier analysis\n",
        "results_df = pd.DataFrame(grid_results)\n",
        "print(\"\\nGrid search results:\")\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Grid Search Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create heatmap of AUC\n",
        "pivot_auc = results_df.pivot(index='sigma', columns='lambda', values='mean_auc')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(pivot_auc, annot=True, fmt='.3f', cmap='viridis', ax=ax, cbar_kws={'label': 'Mean AUC'})\n",
        "ax.set_title('Cross-Validation AUC by Hyperparameters')\n",
        "ax.set_xlabel('Lambda (Regularization)')\n",
        "ax.set_ylabel('Sigma (Kernel Bandwidth)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best parameters\n",
        "best_idx = results_df['mean_auc'].idxmax()\n",
        "best_params = results_df.loc[best_idx]\n",
        "\n",
        "print(\"Best hyperparameters:\")\n",
        "print(f\"  Sigma: {best_params['sigma']:.2f}\")\n",
        "print(f\"  Lambda: {best_params['lambda']:.2f}\")\n",
        "print(f\"  Mean AUC: {best_params['mean_auc']:.3f} ± {best_params['std_auc']:.3f}\")\n",
        "print(f\"  Mean Accuracy: {best_params['mean_accuracy']:.3f}\")\n",
        "print(f\"  Mean Kappa: {best_params['mean_kappa']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Fit Final Model with Best Parameters\n",
        "\n",
        "Train the final model using the best hyperparameters found via grid search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit final model with best parameters\n",
        "final_model = KLRfome(\n",
        "    sigma=best_params['sigma'],\n",
        "    lambda_reg=best_params['lambda'],\n",
        "    n_rff_features=256,\n",
        "    window_size=3,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "final_model.fit(training_data)\n",
        "\n",
        "print(f\"Final model fitted:\")\n",
        "print(f\"  Converged: {final_model._fit_result.converged}\")\n",
        "print(f\"  Iterations: {final_model._fit_result.n_iterations}\")\n",
        "print(f\"  Final loss: {final_model._fit_result.final_loss:.6f}\")\n",
        "\n",
        "# Compare to default parameters\n",
        "default_model = KLRfome(sigma=1.0, lambda_reg=0.1, n_rff_features=256, window_size=3, seed=SEED)\n",
        "default_model.fit(training_data)\n",
        "\n",
        "print(f\"\\nDefault model (sigma=1.0, lambda=0.1):\")\n",
        "print(f\"  Converged: {default_model._fit_result.converged}\")\n",
        "print(f\"  Iterations: {default_model._fit_result.n_iterations}\")\n",
        "print(f\"  Final loss: {default_model._fit_result.final_loss:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Understanding Validation Metrics\n",
        "\n",
        "KLRfome provides comprehensive metrics for model evaluation. Let's explore the key metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from klrfome.utils.validation import metrics, CM_quads\n",
        "\n",
        "# Get metrics from best fold\n",
        "best_fold_result = next(f for f in cv_results['folds'] if f['fold'] == cv_results['best_fold'])\n",
        "best_metrics = best_fold_result['metrics']\n",
        "\n",
        "print(\"Key Metrics Explained:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"AUC (Area Under ROC Curve): {best_metrics['AUC']:.3f}\")\n",
        "print(\"  - Measures ability to distinguish between classes\")\n",
        "print(\"  - Range: 0.5 (random) to 1.0 (perfect)\")\n",
        "print(\"  - >0.7: acceptable, >0.8: good, >0.9: excellent\")\n",
        "print()\n",
        "\n",
        "print(f\"Accuracy: {best_metrics['Accuracy']:.3f}\")\n",
        "print(\"  - Overall proportion of correct predictions\")\n",
        "print()\n",
        "\n",
        "print(f\"Sensitivity (Recall, TPR): {best_metrics['Sensitivity']:.3f}\")\n",
        "print(\"  - Proportion of actual sites correctly identified\")\n",
        "print(\"  - True Positive Rate\")\n",
        "print()\n",
        "\n",
        "print(f\"Specificity (TNR): {best_metrics['Specificity']:.3f}\")\n",
        "print(\"  - Proportion of actual background correctly identified\")\n",
        "print(\"  - True Negative Rate\")\n",
        "print()\n",
        "\n",
        "print(f\"Precision (PPV): {best_metrics['Precision']:.3f}\")\n",
        "print(\"  - Proportion of predicted sites that are actually sites\")\n",
        "print()\n",
        "\n",
        "print(f\"Kappa: {best_metrics['Kappa']:.3f}\")\n",
        "print(\"  - Agreement between predictions and observations\")\n",
        "print(\"  - Accounts for chance agreement\")\n",
        "print(\"  - Range: -1 to 1, >0.6: good agreement\")\n",
        "print()\n",
        "\n",
        "print(f\"F-Measure: {best_metrics['F_Measure']:.3f}\")\n",
        "print(\"  - Harmonic mean of precision and recall\")\n",
        "print(\"  - Balances precision and sensitivity\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "The confusion matrix shows the breakdown of predictions vs. observations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display confusion matrix for best fold\n",
        "print(f\"Confusion Matrix (Fold {cv_results['best_fold']}):\")\n",
        "print(f\"  True Positives (TP):  {best_fold_result['TP']}\")\n",
        "print(f\"  False Positives (FP): {best_fold_result['FP']}\")\n",
        "print(f\"  True Negatives (TN):  {best_fold_result['TN']}\")\n",
        "print(f\"  False Negatives (FN): {best_fold_result['FN']}\")\n",
        "print()\n",
        "\n",
        "# Visualize confusion matrix\n",
        "cm_data = np.array([\n",
        "    [best_fold_result['TP'], best_fold_result['FP']],\n",
        "    [best_fold_result['FN'], best_fold_result['TN']]\n",
        "])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "sns.heatmap(cm_data, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "            xticklabels=['Predicted Site', 'Predicted Background'],\n",
        "            yticklabels=['Actual Site', 'Actual Background'])\n",
        "ax.set_title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This tutorial demonstrated:\n",
        "\n",
        "1. **Cross-Validation**: Use `model.cross_validate()` to evaluate model performance\n",
        "2. **Grid Search**: Systematically search parameter space to find optimal hyperparameters\n",
        "3. **Model Selection**: Choose best parameters based on CV metrics (typically AUC)\n",
        "4. **Metrics Interpretation**: Understand key validation metrics\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "- **AUC** is often the best metric for model selection (handles class imbalance)\n",
        "- **Sigma** controls kernel bandwidth (smoothness of predictions)\n",
        "- **Lambda** controls regularization (prevents overfitting)\n",
        "- Use **stratified CV** to maintain class balance across folds\n",
        "- **5-fold CV** is a good default for most datasets\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Use the best parameters to fit your final model\n",
        "- Generate predictions on your full dataset\n",
        "- Validate on independent test data if available\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
